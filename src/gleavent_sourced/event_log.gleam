import gleam/result
import gleam/dict
import gleam/dynamic.{type Dynamic}
import gleam/dynamic/decode
import gleam/json
import gleam/list


import gleam/erlang/process
import gleavent_sourced/event_filter.{type EventFilter}
import gleavent_sourced/parrot_pog
import gleavent_sourced/sql
import pog

/// Custom error type for query operations that can fail at database or mapping level
pub type QueryError {
  DatabaseError(pog.QueryError)
  JsonParseError(json.DecodeError)
  MappingError(String)
}

// Event type for reading events from the database
// sequence_number is auto-generated by PostgreSQL
pub type Event {
  Event(
    sequence_number: Int,
    occurred_at: String,
    event_type: String,
    payload: dynamic.Dynamic,
    metadata: dict.Dict(String, String),
  )
}

/// Result type for append operations with optimistic concurrency control
pub type AppendResult {
  AppendSuccess
  AppendConflict(conflict_count: Int)
}

pub fn connect(pool_name: process.Name(pog.Message)) -> pog.Connection {
  pog.named_connection(pool_name)
}

pub fn append_events(
  db: pog.Connection,
  events: List(event_type),
  event_converter: fn(event_type) -> #(String, json.Json),
  metadata: dict.Dict(String, String),
  conflict_filter: EventFilter,
  last_seen_sequence: Int,
) -> Result(AppendResult, pog.QueryError) {
  // Convert events to JSON array format
  let events_json_array =
    events
    |> json.array(fn(event) {
      let #(event_type, payload_json) = event_converter(event)
      json.object([
        #("type", json.string(event_type)),
        #("data", payload_json),
        #("metadata", json.object(
          dict.to_list(metadata)
          |> list.map(fn(pair) { #(pair.0, json.string(pair.1)) })
        ))
      ])
    })
    |> json.to_string

  // Convert conflict filter to JSON string
  let conflict_filter_json = event_filter.to_string(conflict_filter)

  // Execute batch insert with conflict check
  let #(batch_sql, batch_params, batch_decoder) =
    sql.batch_insert_events_with_conflict_check(
      conflict_filter: conflict_filter_json,
      last_seen_sequence: last_seen_sequence,
      events: events_json_array
    )

  let batch_query =
    pog.query(batch_sql)
    |> parrot_pog.parameters(batch_params)
    |> pog.returning(batch_decoder)

  pog.execute(batch_query, on: db)
  |> result.map(fn(returned){
    let assert [row] = returned.rows
    case row.status {
      "success" -> AppendSuccess
      "conflict" -> AppendConflict(conflict_count: row.conflict_count)
      _ -> panic as "unexpected status from batch insert"
    }
  })
}

/// Query events using EventFilter with typed result mapping
pub fn query_events(
  db: pog.Connection,
  filter: EventFilter,
  event_mapper: fn(String, Dynamic) -> Result(event_type, String),
) -> Result(#(List(event_type), Int), QueryError) {
  let filters_json = event_filter.to_string(filter)

  let #(select_sql, select_params, _decoder) =
    sql.read_events_with_filter(filters: filters_json)

  let select_query =
    pog.query(select_sql)
    |> parrot_pog.parameters(select_params)
    |> pog.returning(sql.read_events_with_filter_decoder())

  case pog.execute(select_query, on: db) {
    Ok(returned) -> {
      let raw_rows = returned.rows

      // Get max sequence number (all rows have the same value)
      let max_sequence = case raw_rows {
        [] -> 0
        [first, ..] -> first.current_max_sequence
      }

      // Map events using the provided mapper
      let mapped_events =
        list.try_map(raw_rows, fn(row) {
          case json.parse(row.payload, decode.dynamic) {
            Ok(payload_dynamic) ->
              case event_mapper(row.event_type, payload_dynamic) {
                Ok(event) -> Ok(event)
                Error(msg) -> Error(MappingError(msg))
              }
            Error(err) -> Error(JsonParseError(err))
          }
        })

      case mapped_events {
        Ok(events) -> Ok(#(events, max_sequence))
        Error(err) -> Error(err)
      }
    }
    Error(err) -> Error(DatabaseError(err))
  }
}
