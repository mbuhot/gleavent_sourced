import gleam/dict
import gleam/dynamic.{type Dynamic}
import gleam/dynamic/decode
import gleam/json
import gleam/list
import gleam/option.{None, Some}
import gleam/result

import gleam/erlang/process
import gleavent_sourced/event_filter.{type EventFilter}
import gleavent_sourced/parrot_pog
import gleavent_sourced/sql
import pog

/// Custom error type for query operations that can fail at database or mapping level
pub type QueryError {
  DatabaseError(pog.QueryError)
  JsonParseError(json.DecodeError)
  MappingError(String)
}

// Event type for reading events from the database
// sequence_number is auto-generated by PostgreSQL
pub type Event {
  Event(
    sequence_number: Int,
    occurred_at: String,
    event_type: String,
    payload: dynamic.Dynamic,
    metadata: dict.Dict(String, String),
  )
}

/// Result type for append operations with optimistic concurrency control
pub type AppendResult {
  AppendSuccess
  AppendConflict(conflict_count: Int)
}

pub fn connect(pool_name: process.Name(pog.Message)) -> pog.Connection {
  pog.named_connection(pool_name)
}

pub fn append_events(
  db: pog.Connection,
  events: List(event_type),
  event_converter: fn(event_type) -> #(String, json.Json),
  metadata: dict.Dict(String, String),
  conflict_filter: EventFilter,
  last_seen_sequence: Int,
) -> Result(AppendResult, pog.QueryError) {
  // Convert events to JSON array format
  let events_json_array =
    events
    |> json.array(fn(event) {
      let #(event_type, payload_json) = event_converter(event)
      json.object([
        #("type", json.string(event_type)),
        #("data", payload_json),
        #(
          "metadata",
          json.object(
            dict.to_list(metadata)
            |> list.map(fn(pair) { #(pair.0, json.string(pair.1)) }),
          ),
        ),
      ])
    })
    |> json.to_string

  // Convert conflict filter to JSON string
  let conflict_filter_json = event_filter.to_string(conflict_filter)

  // Execute batch insert with conflict check
  let #(batch_sql, batch_params, batch_decoder) =
    sql.batch_insert_events_with_conflict_check(
      conflict_filter: conflict_filter_json,
      last_seen_sequence: last_seen_sequence,
      events: events_json_array,
    )

  let batch_query =
    pog.query(batch_sql)
    |> parrot_pog.parameters(batch_params)
    |> pog.returning(batch_decoder)

  pog.execute(batch_query, on: db)
  |> result.map(fn(returned) {
    let assert [row] = returned.rows
    case row.status {
      "success" -> AppendSuccess
      "conflict" -> AppendConflict(conflict_count: row.conflict_count)
      _ -> panic as "unexpected status from batch insert"
    }
  })
}

/// Query events using EventFilter with typed result mapping
pub fn query_events(
  db: pog.Connection,
  filter: EventFilter,
  event_mapper: fn(String, Dynamic) -> Result(event_type, String),
) -> Result(#(List(event_type), Int), QueryError) {
  let filters_json = event_filter.to_string(filter)

  let #(select_sql, select_params, _decoder) =
    sql.read_events_with_filter(filters: filters_json)

  let select_query =
    pog.query(select_sql)
    |> parrot_pog.parameters(select_params)
    |> pog.returning(sql.read_events_with_filter_decoder())

  case pog.execute(select_query, on: db) {
    Ok(returned) -> {
      let raw_rows = returned.rows

      // Get max sequence number (all rows have the same value)
      let max_sequence = case raw_rows {
        [] -> 0
        [first, ..] -> first.current_max_sequence
      }

      // Map events using the provided mapper
      let mapped_events =
        list.try_map(raw_rows, fn(row) {
          case json.parse(row.payload, decode.dynamic) {
            Ok(payload_dynamic) ->
              case event_mapper(row.event_type, payload_dynamic) {
                Ok(event) -> Ok(event)
                Error(msg) -> Error(MappingError(msg))
              }
            Error(err) -> Error(JsonParseError(err))
          }
        })

      case mapped_events {
        Ok(events) -> Ok(#(events, max_sequence))
        Error(err) -> Error(err)
      }
    }
    Error(err) -> Error(DatabaseError(err))
  }
}

/// Query events with SQL-level fact tagging and return events grouped by fact ID
pub fn query_events_with_tags(
  db: pog.Connection,
  filter: EventFilter,
  event_mapper: fn(String, Dynamic) -> Result(event_type, String),
) -> Result(#(dict.Dict(String, List(event_type)), Int), QueryError) {
  let filters_json = event_filter.to_string(filter)

  let #(select_sql, select_params, decoder) =
    sql.read_events_with_fact_tags(filters: filters_json)

  let select_query =
    pog.query(select_sql)
    |> parrot_pog.parameters(select_params)
    |> pog.returning(decoder)

  case pog.execute(select_query, on: db) {
    Ok(returned) -> {
      let raw_rows = returned.rows

      // Get max sequence number (all rows have the same value)
      let max_sequence = case raw_rows {
        [] -> 0
        [first, ..] -> first.current_max_sequence
      }

      // Group events by fact ID using matching_facts JSON
      let events_by_fact_result =
        list.try_fold(raw_rows, dict.new(), fn(acc, row) {
          case json.parse(row.payload, decode.dynamic) {
            Ok(payload_dynamic) ->
              case event_mapper(row.event_type, payload_dynamic) {
                Ok(event) ->
                  case
                    json.parse(row.matching_facts, decode.list(decode.string))
                  {
                    Ok(fact_ids) -> {
                      let updated_dict =
                        list.fold(fact_ids, acc, fn(dict_acc, fact_id) {
                          dict.upsert(dict_acc, fact_id, fn(existing_events) {
                            case existing_events {
                              Some(events) -> [event, ..events]
                              None -> [event]
                            }
                          })
                        })
                      Ok(updated_dict)
                    }
                    Error(_) ->
                      Error(MappingError("Failed to parse matching_facts JSON"))
                  }
                Error(msg) -> Error(MappingError(msg))
              }
            Error(err) -> Error(JsonParseError(err))
          }
        })

      case events_by_fact_result {
        Ok(events_by_fact) -> {
          // Reverse event lists to maintain chronological order
          let ordered_events_by_fact =
            dict.map_values(events_by_fact, fn(_, events) {
              list.reverse(events)
            })
          Ok(#(ordered_events_by_fact, max_sequence))
        }
        Error(err) -> Error(err)
      }
    }
    Error(err) -> Error(DatabaseError(err))
  }
}
