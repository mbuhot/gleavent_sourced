import gleam/dict
import gleam/dynamic.{type Dynamic}
import gleam/dynamic/decode
import gleam/int
import gleam/json
import gleam/list
import gleam/option.{None, Some}
import gleam/result
import gleam/string

import gleam/erlang/process
import gleam/time/timestamp.{type Timestamp}
import gleavent_sourced/event_filter.{type EventFilter}
import pog

/// Custom error type for query operations that can fail at database or mapping level
pub type QueryError {
  DatabaseError(pog.QueryError)
  JsonParseError(json.DecodeError)
  MappingError(String)
}

// Event type for reading events from the database
// sequence_number is auto-generated by PostgreSQL
pub type Event {
  Event(
    sequence_number: Int,
    occurred_at: Timestamp,
    event_type: String,
    payload: dynamic.Dynamic,
    metadata: dict.Dict(String, String),
  )
}

/// Result type for append operations with optimistic concurrency control
pub type AppendResult {
  AppendSuccess
  AppendConflict(conflict_count: Int)
}

pub fn connect(pool_name: process.Name(pog.Message)) -> pog.Connection {
  pog.named_connection(pool_name)
}

pub fn append_events(
  db: pog.Connection,
  events: List(event_type),
  event_converter: fn(event_type) -> #(String, json.Json),
  metadata: dict.Dict(String, String),
  conflict_filter: EventFilter,
  last_seen_sequence: Int,
) -> Result(AppendResult, pog.QueryError) {
  // Convert events to JSON array format
  let events_json_array =
    events
    |> json.array(fn(event) {
      let #(event_type, payload_json) = event_converter(event)
      json.object([
        #("type", json.string(event_type)),
        #("data", payload_json),
        #(
          "metadata",
          json.object(
            dict.to_list(metadata)
            |> list.map(fn(pair) { #(pair.0, json.string(pair.1)) }),
          ),
        ),
      ])
    })
    |> json.to_string

  // Generate dynamic SQL with conflict checking
  let sql_and_params =
    build_dynamic_insert_sql(
      conflict_filter.filters,
      last_seen_sequence,
      events_json_array,
    )

  let batch_query =
    pog.query(sql_and_params.sql)
    |> list.fold(sql_and_params.params, _, fn(query, param) {
      pog.parameter(query, param)
    })
    |> pog.returning(append_result_decoder())

  pog.execute(batch_query, on: db)
  |> result.map(fn(returned) {
    let assert [row] = returned.rows
    let #(status, conflict_count) = row
    case status {
      "success" -> AppendSuccess
      "conflict" -> AppendConflict(conflict_count: conflict_count)
      _ -> panic as "unexpected status from batch insert"
    }
  })
}

/// Query events with dynamic SQL generation for fact tagging
/// Returns events grouped by fact ID
pub fn query_events_with_tags(
  db: pog.Connection,
  filter: EventFilter,
  event_mapper: fn(String, Dynamic) -> Result(event_type, String),
) -> Result(#(dict.Dict(String, List(event_type)), Int), QueryError) {
  // Generate dynamic SQL based on filter conditions
  let sql_and_params = build_dynamic_query_sql(filter.filters)
  let select_query =
    pog.query(sql_and_params.sql)
    |> list.fold(sql_and_params.params, _, fn(query, param) {
      pog.parameter(query, param)
    })
    |> pog.returning(dynamic_query_decoder())

  case pog.execute(select_query, on: db) {
    Ok(returned) -> {
      let raw_rows = returned.rows

      // Get max sequence number (all rows have the same value)
      let max_sequence = case raw_rows {
        [] -> 0
        [first, ..] -> first.max_sequence_number
      }

      // Group events by fact ID and map them
      list.try_map(raw_rows, fn(row) {
        case json.parse(row.payload, decode.dynamic) {
          Ok(payload_dynamic) ->
            case event_mapper(row.event_type, payload_dynamic) {
              Ok(event) -> Ok(#(row.fact_id, event))
              Error(msg) -> Error(MappingError(msg))
            }
          Error(err) -> Error(JsonParseError(err))
        }
      })
      |> result.map(fn(events) {
        list.group(events, fn(pr) { pr.0 })
        |> dict.map_values(fn(_k, pairs) { list.map(pairs, fn(pr) { pr.1 }) })
      })
      |> result.map(fn(events_dict) { #(events_dict, max_sequence) })
    }
    Error(err) -> Error(DatabaseError(err))
  }
}

/// Result type for dynamic query results
type DynamicQueryResult {
  DynamicQueryResult(
    fact_id: String,
    sequence_number: Int,
    event_type: String,
    payload: String,
    metadata: String,
    max_sequence_number: Int,
  )
}

/// Helper type for dynamic SQL generation
type SqlAndParams {
  SqlAndParams(sql: String, params: List(pog.Value))
}

/// Build a single CTE for a filter condition
fn build_condition_cte(
  condition: event_filter.FilterCondition,
  index: Int,
  param_offset: Int,
  select_clause: String,
  extra_where: String,
) -> SqlAndParams {
  let cte_name = "condition" <> int.to_string(index) <> "_events"
  let param_index_event_type = index * 2 + param_offset + 1
  let param_index_payload = index * 2 + param_offset + 2

  let cte =
    cte_name
    <> " AS ("
    <> "  SELECT "
    <> select_clause
    <> "  FROM events e"
    <> "  WHERE e.event_type = $"
    <> int.to_string(param_index_event_type)
    <> "    AND e.payload @> $"
    <> int.to_string(param_index_payload)
    <> "::jsonb"
    <> extra_where
    <> ")"

  let params = [
    pog.text(condition.event_type),
    pog.text(json.to_string(condition.filter_expr)),
  ]

  SqlAndParams(sql: cte, params: params)
}

/// Build all CTEs from conditions
fn build_condition_ctes(
  conditions: List(event_filter.FilterCondition),
  param_offset: Int,
  select_clause: String,
  extra_where: String,
) -> #(List(String), List(pog.Value)) {
  let indexed_conditions =
    list.index_map(conditions, fn(condition, index) { #(condition, index) })

  let results =
    list.map(indexed_conditions, fn(pair) {
      let #(condition, index) = pair
      build_condition_cte(
        condition,
        index,
        param_offset,
        select_clause,
        extra_where,
      )
    })

  let ctes = list.map(results, fn(result) { result.sql })
  let params = list.flatten(list.map(results, fn(result) { result.params }))
  #(ctes, params)
}

/// Build UNION clause from condition indices
fn build_union_clause(condition_count: Int) -> String {
  let cte_names =
    list.range(0, condition_count - 1)
    |> list.map(fn(index) { "condition" <> int.to_string(index) <> "_events" })

  case cte_names {
    [] -> ""
    [single] -> single
    multiple -> string.join(multiple, " UNION SELECT * FROM ")
  }
}

/// Build dynamic SQL query with CTEs for each filter condition
fn build_dynamic_query_sql(
  conditions: List(event_filter.FilterCondition),
) -> SqlAndParams {
  case conditions {
    [] ->
      SqlAndParams(
        sql: "SELECT NULL as fact_id, sequence_number, event_type, payload, metadata, 0 as max_sequence_number FROM events WHERE false",
        params: []
      )
    _ -> {
      // Build CTEs with fact_id selection
      let #(ctes, params) = {
        let indexed_conditions =
          list.index_map(conditions, fn(condition, index) {
            #(condition, index)
          })
        let results =
          list.map(indexed_conditions, fn(pair) {
            let #(condition, index) = pair
            let fact_id = case condition.tag {
              Some(tag) -> "'" <> tag <> "'"
              None -> "'" <> condition.event_type <> "'"
            }
            let select_clause = fact_id <> " as fact_id, e.sequence_number, e.event_type, e.payload, e.metadata"
            build_condition_cte(condition, index, 0, select_clause, "")
          })

        let ctes = list.map(results, fn(result) { result.sql })
        let params =
          list.flatten(list.map(results, fn(result) { result.params }))
        #(ctes, params)
      }

      let union_clause =
        "all_events AS ("
        <> "  SELECT * FROM "
        <> build_union_clause(list.length(conditions))
        <> ")"

      let final_sql =
        "WITH "
        <> string.join(ctes, ", ")
        <> ", "
        <> union_clause
        <> " SELECT e.fact_id, e.sequence_number, e.event_type, e.payload, e.metadata,"
        <> " MAX(e.sequence_number) OVER () as max_sequence_number"
        <> " FROM all_events e"
        <> " ORDER BY e.sequence_number"

      SqlAndParams(sql: final_sql, params: params)
    }
  }
}

/// Build dynamic SQL INSERT with conflict checking
fn build_dynamic_insert_sql(
  conditions: List(event_filter.FilterCondition),
  last_seen_sequence: Int,
  events_json_array: String,
) -> SqlAndParams {
  case conditions {
    [] ->
      // No conflict conditions, just insert directly
      SqlAndParams(
        sql: "WITH new_events_parsed AS (
          SELECT
            event_data ->> 'type' as event_type,
            event_data -> 'data' as event_data,
            event_data -> 'metadata' as metadata
          FROM jsonb_array_elements($1) AS event_data
        ),
        insert_result AS (
          INSERT INTO events (event_type, payload, metadata)
          SELECT event_type, event_data, metadata
          FROM new_events_parsed
          RETURNING sequence_number
        )
        SELECT 'success' as status, 0 as conflict_count
        FROM (SELECT 1) dummy
        WHERE EXISTS(SELECT 1 FROM insert_result)",
        params: [pog.text(events_json_array)],
      )
    _ -> {
      // Build CTEs for conflict checking
      let extra_where =
        "    AND e.sequence_number > " <> int.to_string(last_seen_sequence)
      let #(ctes, conflict_params) =
        build_condition_ctes(conditions, 1, "e.sequence_number", extra_where)

      let conflict_union = build_union_clause(list.length(conditions))

      let final_sql =
        "WITH "
        <> string.join(ctes, ", ")
        <> ", conflict_check (conflict_count) AS ("
        <> "  SELECT COUNT(*) FROM ("
        <> "    SELECT sequence_number FROM "
        <> conflict_union
        <> "  ) conflicts"
        <> "), new_events_parsed AS ("
        <> "  SELECT"
        <> "    event_data ->> 'type' as event_type,"
        <> "    event_data -> 'data' as event_data,"
        <> "    event_data -> 'metadata' as metadata"
        <> "  FROM jsonb_array_elements($1) AS event_data"
        <> "), insert_result AS ("
        <> "  INSERT INTO events (event_type, payload, metadata)"
        <> "  SELECT event_type, event_data, metadata"
        <> "  FROM new_events_parsed"
        <> "  WHERE (SELECT conflict_count FROM conflict_check) = 0"
        <> "  RETURNING sequence_number"
        <> ")"
        <> "SELECT"
        <> "  CASE"
        <> "    WHEN EXISTS(SELECT 1 FROM insert_result) THEN 'success'"
        <> "    ELSE 'conflict'"
        <> "  END as status,"
        <> "  (SELECT conflict_count FROM conflict_check) as conflict_count"

      let all_params = [pog.text(events_json_array), ..conflict_params]
      SqlAndParams(sql: final_sql, params: all_params)
    }
  }
}

/// Decoder for append results
fn append_result_decoder() -> decode.Decoder(#(String, Int)) {
  use status <- decode.field(0, decode.string)
  use conflict_count <- decode.field(1, decode.int)
  decode.success(#(status, conflict_count))
}

/// Decoder for dynamic query results
fn dynamic_query_decoder() -> decode.Decoder(DynamicQueryResult) {
  use fact_id <- decode.field(0, decode.string)
  use sequence_number <- decode.field(1, decode.int)
  use event_type <- decode.field(2, decode.string)
  use payload <- decode.field(3, decode.string)
  use metadata <- decode.field(4, decode.string)
  use max_sequence_number <- decode.field(5, decode.int)
  decode.success(DynamicQueryResult(
    fact_id:,
    sequence_number:,
    event_type:,
    payload:,
    metadata:,
    max_sequence_number:,
  ))
}
